% Chapter Template

\chapter{Discussion} % Main chapter title

\label{Chapter7} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Whole Proof Generation}
\label{wholeproof}

The authors were quite surprised to find out that performance on mobile ARM processors is almost as good as x86-64 i7 performance. Performance on the i7 processor is still better, however ARM's 8 cores and higher price tag make the performance gap really narrow.\\
\\
Another interesting insight is the importance of the processor word width. 32-bit code was slower than its 64-bit counterpart on both architectures. This performance penalty is incurred because of arithmetic operations. zk-SNARKs rely on 381 bit finite prime field arithmetic. Emulating these operations requires fewer 64-bit than 32-bit operations. For 32-bit code every 64-bit addition needs to be decomposed into 2 32-bit additions. Every multiplication requires 3 32-bit multiplications (4 if we want to keep the high bits). On top of this, there are several LEA/ADD instructions needed to compute the final product.\\
\\
The stress test showed that phones are prone to overheating, and throttle the computation of proofs if under heavy load. This happens after several proofs have been computed, even if nothing else is running on the device. Laptops don't show this behavior due to the fans disperse the heat. Note that the test was performed on laptop with GPU shut down.

\section{FFT and Multiexp}

The results of this test showed no surprising facts. Multiexponentiation and FFT are the most time consuming parts of zk-SNARKs. Parallelizing FFT would lead to minimal performance gains due to it taking only 15\% of execution time. Considering that the simplified Pippinger's algorithm used for multiexponentiation is inherently parallelizable, it lent itself as the perfect candidate for GPU acceleration.

\section{131k Test}

Integrated GPUs performed poorly when compared to CPUs and discrete GPUs. This rules out the use of mobile GPUs for acceleration of zk-SNARKs. If we take a look at frequencies and core counts of discrete and integrated GPUs, it is clear that Intel and Mali GPUs will be at least an order of magnitude slower than NVIDIA and ATI cards. The actual surprise is that not even discrete GPUs can beat CPUs running Pippinger's algorithm. In this section we explain multiple contributing factors.\\
\\
\subsection{Processor Word Length - 32 vs 64}

As we concluded in section \ref{wholeproof}, processor bitness plays an important role in zk-SNARK performance. With GPUs, this effect is further amplified by the memory hierarchy. All of today's GPUs are still clusters of 32-bit RISC cores operating in unison. Small word length means that these cores need to execute several times more instructions than CPUs running equivalent code.\\

\subsection{Branch Divergence}

A common problem with code running on GPUs is branch divergence. When CPU encounters a branch in the code, it will execute only one of two code paths. Unfortunately, all GPU cores in a group need to execute the same instruction. To satisfy this, GPUs will execute both branches, and tell cores to ignore the instructions which don't satisfy their condition check.\\
\\
However, profiling on our ATI card showed thread convergence rate of 85\%. Improving this further would lead to minimal gains in performance.

\subsection{Inlining, Loops and Constants}

While functions are considered good practice on CPUs, using them on GPUs can significantly hinder performance. Function calls require a call stack, just like on CPUs. Unfortunately, this stack