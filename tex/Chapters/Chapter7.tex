% Chapter Template

\chapter{Discussion} % Main chapter title

\label{Chapter7} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Whole Proof Generation}
\label{wholeproof}

The authors were quite surprised to find out that performance on mobile ARM processors is almost as good as x86-64 i7 performance. Performance on the i7 processor is still better, however ARM's 8 cores and higher price tag make the performance gap really narrow.\\
\\
Another interesting insight is the importance of the processor word width. 32-bit code was slower than its 64-bit counterpart on both architectures. This performance penalty is incurred because of arithmetic operations. zk-SNARKs rely on 381 bit finite prime field arithmetic. Emulating these operations requires fewer 64-bit than 32-bit operations. For 32-bit code every 64-bit addition needs to be decomposed into 2 32-bit additions. Every multiplication requires 3 32-bit multiplications (4 if we want to keep the high bits). On top of this, there are several LEA/ADD instructions needed to compute the final product.\\
\\
The stress test showed that phones are prone to overheating, and throttle the computation of proofs if under heavy load. This happens after several proofs have been computed, even if nothing else is running on the device. Laptops don't show this behavior due to the fans disperse the heat. Note that the test was performed on laptop with GPU shut down.

\section{FFT and Multiexp}

The results of this test showed no surprising facts. Multiexponentiation and FFT are the most time consuming parts of zk-SNARKs. Parallelizing FFT would lead to minimal performance gains due to it taking only 15\% of execution time. Considering that the simplified Pippinger's algorithm used for multiexponentiation is inherently parallelizable, it lent itself as the perfect candidate for GPU acceleration.

\section{131k Test}

Integrated GPUs performed poorly when compared to CPUs and discrete GPUs. This rules out the use of mobile GPUs for acceleration of zk-SNARKs. If we take a look at frequencies and core counts of discrete and integrated GPUs, it is clear that Intel and Mali GPUs will be at least an order of magnitude slower than NVIDIA and ATI cards. The actual surprise is that not even discrete GPUs can beat CPUs running Pippinger's algorithm. In this section we explain multiple contributing factors.\\
\\
\subsection{Processor Word Length - 32 vs 64}

As we concluded in section \ref{wholeproof}, processor word length plays an important role in zk-SNARK performance. With GPUs, this effect is further amplified by the memory hierarchy. All of today's GPUs are still clusters of 32-bit RISC cores operating in unison. Small word length means that these cores need to execute several times more instructions than CPUs running equivalent code.\\

\subsection{Branch Divergence}

A common problem with code running on GPUs is branch divergence. When CPU encounters a branch in the code, it will execute only one of two code paths. Unfortunately, all GPU cores in a group need to execute the same instruction. To satisfy this, GPUs will execute both branches, and tell cores to ignore the instructions which don't satisfy their condition check.\\
\\
However, profiling on our ATI card showed thread convergence rate of 85\%. Improving this further would lead to minimal gains in performance.

\subsection{Inlining, Loops and Constants}

While functions are considered good practice on CPUs, using them on GPUs can significantly hinder performance. Function calls require a call stack. Unfortunately, this stack is stored in global memory. For small kernels this isn't a problem because stack can be cached. However, our kernels have elliptic curve points as arguments, so the stack is too big to fit in cache. Another problem is that Pippinger's algorithm requires an array of elliptic curve points with random access. Kernel disassembly of NVIDIA code showed that these arrays are stored on stack.\\
\\
Inlining function calls and unwrapping loops reduces divergence and removes the stack. However, it substantially increases the size of code. This,combined with 32-bit registers, results in code that is too large to fit in instruction cache, incurring a performance penalty. Analysis of ATI disasembly shows that the kernels were around 90 KB in size, while the instruction cache has only 30 KB. NVIDIA compiler actively tried to prevent code explosion by keeping the functions, even though they had been marked inline. Unfortunately, this resulted in a large call stack.\\
\\
OpenCL uses global memory to store constants. Usually, they can be cached, so performance penalty doesn't need to be high. However, if the kernel has a lot of memory traffic, constants will be pushed out of the cache, and will be fetched every time they are used. To prvent this, we used macros in place of variables, hoping that compilers will use immediate load instructions instead of memory reads. Unfortunately, both NVIDIA and ATI compilers detected constants and put them in global memory.\\
\\
All of these things, combined with high register usage of kernels (more than 160 registers on NVIDIA, 256 on ATI), results in high memory traffic for Pippinger's algorithm on GPUs. FIGURE shows data from ATI profiler - vector ALU usage and memory unit usage for binary method and Pippinger's algorithm. Even though Pippinger's algorithm is faster, it barely uses ALU and has relatively high memory usage. CPUs can run the same algorithm with much larger array sizes without incurring a performance penalty, leading to better performance.